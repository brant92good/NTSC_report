\section{「預訓練—微調」範式下之代表性模型 (Representative Models under the Pre-train, Fine-tune Paradigm)}

本節將回顧目前在傳統「預訓練—微調（Pre-train, Fine-tune）」範式下在語音深偽偵測任務上的代表性模型。在此範式下，普遍以透過在大規模資料配合自監督學習方式預訓練的模型作為前端模型，串接為了特定下游任務設計的模型結構，在整體上進行參數微調。此範式在各式下游任務上建立了性能標竿，但有著需要龐大存儲空間以及算力資源的短板。以下將詳細介紹兩個在深偽音訊偵測上以及\textit{語者驗證（ASV）}任務上具代表性的架構：基於圖神經網路的 AASIST，以及基於Conformer架構與多尺度特徵融合（MFA）的MFA-Conformer。

\subsection{AASIST模型}

AASIST架構是由Tak et al. (2022)所提出的，在ASVspoof21 LA挑戰中取得優異的成績。該作者在後續研究中將前端模型替換為wav2vec 2.0 XLSR並加上Rawboost Data Augmentation後被視為語音深偽偵測任務的SOTA模型。(許多後續研究以及挑戰基於此模型進行改進語比較，足見此架構之經典)

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/AASIST_model.png}
    \caption{AASIST 模型架構圖}
\end{figure}

該模型的運作流程如下（架構如圖X所示）：
\begin{enumerate}
    \item \textbf{前端特徵提取}：使用預訓練的 \textbf{wav2vec 2.0} 模型對原始音波進行處理。由於 wav2vec 2.0 在大量語音數據上進行了自監督訓練，其提取的特徵具備高度的泛化能力與豐富的聲學細節。
    \item \textbf{深層編碼}：將前端輸出輸入至基於 \textbf{RawNet2} 的殘差編碼器，進一步學習深層的聲學特徵。
    \item \textbf{特徵聚合與圖建模}：透過基於自注意力的聚合層，分別提取\textbf{頻譜（Spectral）}和\textbf{時序（Temporal）}兩種維度的特徵。這兩組特徵隨後各自通過由圖注意網路（GAT）和圖池化（Graph Pooling）組成的圖模塊。
    \item \textbf{異質圖融合}：利用圖結合技術，將頻譜與時序特徵融合為\textbf{異質譜時圖（Heterogeneous Spectro-Temporal Graph）}。隨後，特徵被送入由 **HS-GAL（Heterogeneous Stacking Graph Attention Layer）** 組成的 **MGO（Max Graph Operation）** 模塊，以捕捉跨域的偽造痕跡。
    \item \textbf{分類決策}：最後，將 MGO 產出的兩組節點堆疊並最大化，通過各節點的最大化和平均運算後串接隱藏全連接層，輸出真偽標籤。
\end{enumerate}

此架構透過全參數微調（Full Fine-tuning）wav2vec 2.0 前端與 AASIST 後端，在 \textbf{ASVspoof 2021} 評測中取得了優異成績，成為該挑戰下的里程碑。此模型的一個主要特徵是其前端模型 wav2vec 2.0 XLS-R 本身擁有約 317M 的巨大參數量，因此在面對未見過的攻擊類型或跨資料集測試（如 ASVspoof 2021 DeepFake 數據集）時，展現了傑出的泛化能力。然而，這也意味著其訓練與推論都需要龐大的計算資源與記憶體空間。

\subsection{MFA-Conformer (Conformer with Transfer Learning)}

另一個值得關注的 SOTA 模型是 MFA-Conformer，該架構由 Zhang 等人（2022）首先應用於聲紋識別（ASV），隨後被證實同樣適用於偽造語音偵測。Conformer 區塊（Conformer Block）的設計初衷在於解決 Transformer 雖擅長捕捉長距離全域依賴（Global Context），卻在提取細微局部特徵（Local Features）上不如卷積神經網路（CNN）的問題。Conformer 透過將卷積模組（Convolution Module）嵌入 Transformer 的自注意力機制與前饋網路之間，成功結合了 CNN 的局部感知能力與 Transformer 的全域建模能力。此架構的優勢最早由 Gulati 等人（2020）\cite{gulati2020conformer} 在自動語音識別（ASR）任務中獲得證實：實驗顯示，在 LibriSpeech 基準測試上，Conformer 能以僅 Transformer 四分之一不到的參數量，達到更優異的辨識率（WER）。MFA-Conformer 在『參數效率（Parameter Efficiency）』上所展現的潛力，不僅證實了輕量化設計的可行性，更為本文後續進行跨範式（Cross-paradigm）的模型效能評比，提供了極具代表性的對照組。

(該說一下Conformer block的數學或是放一張Conformer block圖)
% --- [圖片插入說明] 圖五 ---
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/MFA-Conformer_model.png}
    \caption{MFA-Conformer 模型架構圖}
\end{figure}


  % \item \textbf{遷移學習 (Transfer Learning)}： (移至實驗說明)
  % 該模型並非從頭訓練，而是利用了在 **NeMo** 工具包提供的 ASR 模型（NEMO STT En Conformer-CTC Small）上訓練的權重進行初始化。該模型是在包含數千小時英語語音的複合資料集上訓練而成。

為了有效捕捉不同層級的特徵，此模型利用多尺度特徵聚合 (MFA) 與池化機制將所有 Conformer 區塊的輸出進行融合，而非僅使用最後一層。其具體運作流程與 ASP 數學定義如下：

\textbf{第一步：特徵拼接與正規化}：
假設模型共有 $L$ 個 Conformer 區塊（本研究中 $L=16$），第 $i$ 層的輸出特徵序列為 $H_i$。首先，將每一個 Conformer 區塊的輸出在特徵維度上進行拼接（Concatenation），並通過層正規化（Layer Normalization）：
\begin{equation}
H_{concat} = \text{LayerNorm}(\text{Concat}(H_1, H_2, ..., H_L))
\end{equation}

\textbf{第二步：注意力統計池化 (Attentive Statistic Pooling, ASP)}：
為了從變長的語音序列中提取固定維度的嵌入向量（Embedding），拼接後的特徵 $H_{concat}$ 會輸入至 ASP 層。假設 $h_t$ 為 $H_{concat}$ 在時間步 $t$ 的特徵向量，ASP 首先透過注意力機制計算每個時間步的權重 $\alpha_t$：

\begin{equation}
e_t = v^T \tanh(W h_t + b)
\end{equation}
\begin{equation}
\alpha_t = \frac{\exp(e_t)}{\sum_{\tau} \exp(e_\tau)}
\end{equation}

其中 $W$ 與 $b$ 為可學習的權重與偏差，$v^T$ 為投影向量。接著，利用權重 $\alpha_t$ 計算加權平均數（Weighted Mean, $\tilde{\mu}$）與加權標準差（Weighted Standard Deviation, $\tilde{\sigma}$）：

\begin{equation}
\tilde{\mu} = \sum_{t} \alpha_t h_t
\end{equation}
\begin{equation}
\tilde{\sigma} = \sqrt{\sum_{t} \alpha_t (h_t - \tilde{\mu})^2}
\end{equation}

最後，將統計量 $\tilde{\mu}$ 與 $\tilde{\sigma}$ 進行拼接，作為最終的語句層級特徵向量 $H_{MFA}$，再經由 Batch Norm 與線性層進行真偽分類：
\begin{equation}
H_{MFA} = \text{Concat}(\tilde{\mu}, \tilde{\sigma})
\end{equation}


\section{Prompting Paradigm}

隨著大規模預訓練模型在語音領域的廣泛應用，傳統的「預訓練—微調（Pre-train, Fine-tune）」範式面臨了嚴重的擴展性挑戰。在此範式下，模型首先在海量無標註數據上進行預訓練以學習通用表徵，隨後針對特定下游任務，需更新並儲存模型內部的全部或大部分參數。然而，隨著模型參數規模邁入數億級甚至更大的量級，為每個任務維護獨立的模型副本，在存儲空間與計算資源上均造成了沉重負擔。

為了解決此問題，Chang 等人 (2023, 2024) 借鑑了自然語言處理 (NLP) 領域的技術，提出了一種基於提示學習（Prompting）範式的參數高效（Parameter-Efficient）\textbf{SpeechPrompt} 架構。該架構的核心在於凍結預訓練語音語言模型的參數，僅透過插入少量可學習的提示向量（Prompt Vectors），將各類語音任務統一重塑為「語音到單元生成（Speech-to-Unit Generation）」任務，從而實現高效的跨任務遷移。下節將介紹 SpeechPrompt 所基於的核心技術：生成式語音語言模型 GSLM，並在後續章節介紹SpeechPrompt的實踐方法。

\subsection{GSLM}
傳統語音處理模型往往依賴大量標註數據，然而 Lakhotia 等人（2021）提出的 GSLM 旨在模擬人類早期的語言習得過程，僅透過原始音訊輸入即可學習聲學與語言特徵，實現無文本的自然語言處理（Textless NLP）。

GSLM 的核心概念是將連續的語音波形離散化，並在此基礎上訓練語言模型。其標準流程包含三個階段：

\begin{enumerate}
    \item \textbf{Speech-to-Unit (S2u)}：利用預訓練的自監督模型（Self-Supervised Learning, SSL）提取語音特徵，並透過 K-means 分群將連續特徵向量量化為離散單元序列（Discrete Units）。
    \item \textbf{Unit-based Language Model (uLM)}：在離散單元上訓練 Transformer Decoder，學習單元間的機率分佈。
    \item \textbf{Unit-to-Speech (u2S)}：將生成的單元序列輸入解碼器，轉回連續的語音波形。
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/GSLM_model.png}
    \caption{GSLM 架構圖}
\end{figure}

GSLM 的語言模型（uLM）採用標準的 Transformer 架構，並以自回歸（Autoregressive）方式進行預訓練。其訓練目標是最大化下一個離散單元的對數似然函數（Log-Likelihood）。

假設一段語音被編碼為長度為 $T$ 的離散單元序列 $u = (u_1, u_2, ..., u_T)$，則模型的訓練損失函數 $\mathcal{L}$ 定義為：

\begin{equation}
\mathcal{L} = - \sum_{t=1}^{T} \log P(u_t | u_{<t}; \theta)
\end{equation}

其中 $u_{<t}$ 表示當前時刻之前的所有單元 $(u_1, ..., u_{t-1})$，$\theta$ 為模型參數。透過最小化此損失函數，模型能夠捕捉語音中長距離的依賴關係與語法結構，建立語言的機率模型。

在訓練數據方面，Lakhotia 等人（2021）使用了 \textbf{LibriLight} 資料集中的 6k 小時無標註「乾淨（clean）」語音進行訓練。這證實了 GSLM 具備從大規模原始音訊中歸納高階語言知識的能力，而無需依賴任何人工轉寫的文本資源。

GSLM 的生成品質高度依賴於前端 S2u 編碼器的特徵提取能力。Lakhotia 等人（2021）系統性地比較了 CPC、wav2vec 2.0 與 HuBERT 三種主流的自監督模型。研究發現，不同的編碼器對下游生成的影響顯著不同。其中，\textbf{HuBERT} 在語音重合成（Resynthesis）與生成（Generation）的各項客觀指標（如 Perplexity）與主觀指標（如人類聽測 MOS）上，往往能取得較佳的平衡，特別是在捕捉語音內容（Content）方面表現穩健。基於文獻中的發現，後續的相關研究（如 SpeechPrompt）傾向於選擇 HuBERT 作為默認的特徵提取器。


\subsection{SpeechPrompt 架構}

SpeechPrompt的核心概念是將預訓練的語音語言模型（如 GSLM）視為凍結 (Frozen) 的黑盒，僅透過在輸入端或模型內部層級中注入極少量（通常小於總量 0.1\%）的可學習「提示向量 (Prompt Vectors)」，來引導 (Prompting) 模型利用其預訓練所獲得的知識來處理多樣化的下游任務。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/SpeechPrompt_model.png}
    \caption{SpeechPrompt 模型架構圖}
\end{figure}

為了更清晰地闡述 SpeechPrompt 的運作機制，我們將架構圖中的關鍵模組進行細部分解與定義。這些模組共同協作，將輸入語音轉換為下游任務的預測結果：

\begin{itemize}
    \item \textbf{SSL Quantizer (自監督語音量化器)}：
    作為系統的輸入前端，此模組負責將連續的語音波形轉換為離散單元序列（Token Sequence）。如 2.1.3 節所述，本研究考量其特徵提取的穩健性，選用預訓練的 \textbf{HuBERT} 模型提取特徵，並透過 K-means 演算法進行量化。這一步驟是將連續訊號「語言化」的關鍵。
    
    \item \textbf{Prompt Vectors (提示向量)}：
    這是 SpeechPrompt 架構中核心的可訓練參數，負責攜帶任務特定的指令資訊。其主要包含前置於輸入層的 Input Prompts 以及嵌入於模型內部的 Deep Prompts。關於這些向量的具體數學定義、維度設定以及在 Attention 機制中的嵌入方式，我們將在 **2.2.3 節** 中進行詳細的形式化描述。
    
    \item \textbf{Frozen Unit-based Language Model (凍結的單元語言模型)}：
    此為整體架構的骨幹（Backbone），以下將簡稱為 uLM，負責理解輸入的語音單元並生成上下文表示（Contextual Representations）。文獻中已探討了多種模型架構，如 GSLM、pGSLM 與 Unit mBART。本計畫選用 GSLM 作為骨幹。在 Prompt Tuning 過程中，uLM 的所有參數保持凍結（Frozen），僅負責推論。這保證了下游任務訓練不會發生「災難性遺忘（Catastrophic Forgetting）」，並大幅降低了運算成本。
    
    \item \textbf{Verbalizer (標籤映射器)}：
    這是針對分類任務（如 ASVspoof）的關鍵組件，負責將語言模型輸出的高維度單元分佈（Vocabulary Distribution）映射到低維度的任務標籤空間（Label Space）。常見的選擇包括：
    \begin{itemize}
        \item \textbf{Fixed Verbalizer (固定映射)}：手動或隨機指定特定單元（Units）代表目標類別（例如指定單元 \#10 代表「真實語音」）。此方法高度依賴人工設計，且要求模型必須透過原有的 uLM 嵌入表（Embedding Table）精確輸出特定的任務標記（Tokens），在性能與靈活性上通常較為受限。
        \item \textbf{Learnable Verbalizer (可學習映射)}：這是一個取代原有嵌入層映射的輕量級線性投影矩陣（Linear Projection），能自動學習並識別哪些單元組合最能代表特定的分類標籤。SpeechPrompt v2 的實驗證實，此方法在多數任務下皆能顯著提升分類的準確度。
    \end{itemize}
\end{itemize}

SpeechPrompt 採用了結合 \textbf{Input Prompt Tuning} 與 \textbf{Deep Prompt Tuning} 的策略，在模型的不同層級注入資訊:

\begin{itemize}

\item \textbf{Input Prompt Tuning (輸入端提示微調)}

最直觀的 Prompt 嵌入方式是在模型的輸入端加入可訓練的向量。假設輸入語音經過 S2u 編碼後的離散單元序列嵌入（Embedding）為 $E(u) = [e(u_1), e(u_2), ..., e(u_T)]$。我們定義一組可訓練的 Prompt 向量 $P^I = [p^I_1, p^I_2, ..., p^I_L]$，其中 $L$ 為 Prompt 的長度。

在輸入層，我們將 Prompt 向量序列前置於語音單元序列之前，形成新的輸入序列 $X$：

\begin{equation}
X = \text{Concat}(P^I, E(u))
\end{equation}

透過此機制，模型在處理語音特徵序列之前，即受到任務特定參數的制約。這些前置向量提供了可學習的上下文資訊（Contextual Information），有效地調節模型對後續輸入的特徵提取與生成行為，使其適應特定的下游任務。

\item \textbf{Deep Prompt Tuning (深層提示微調)}

為了增強 Prompt 對模型的控制力，SpeechPrompt 進一步採用了 Deep Prompt Tuning 技術。不同於僅在輸入層加入 Prompt，此方法在 Transformer 的每一層 Attention 機制中都嵌入了可訓練的向量。

假設 Transformer 第 $j$ 層的輸入為隱藏狀態 $h$，標準的 Self-Attention 機制計算如下：

\begin{equation}
Attn(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

其中 $Q, K, V$ 分別為 Query, Key, Value 矩陣。在 Deep Prompt Tuning 中，我們引入兩組可訓練的 Prompt 向量 $P_K$ 與 $P_V$，並將其同樣前置於原始的 Key 和 Value 矩陣之前：

\begin{equation}
K' = \text{Concat}(P_K, h)W_K
\end{equation}

\begin{equation}
V' = \text{Concat}(P_V, h)W_V
\end{equation}

透過這種方式，Prompt 向量 $P_K$ 與 $P_V$ 能夠直接影響每一層 Attention 的權重分佈與輸出值。這意味著我們僅需訓練極少量的參數（即 $P^I, P_K, P_V$），就能在不改變模型權重的情況下，有效地「重新程式化（Reprogramming）」模型的行為以適應下游任務。
\end{itemize}

SpeechPrompt 架構根據下游任務的性質，採用不同的訓練目標。針對語音分類任務（如 ASVspoof），模型輸出的離散單元機率分佈會通過 Learnable Verbalizer 投影到類別空間，並透過最小化交叉熵損失函數（Cross-Entropy Loss）來優化 Prompt 向量與 Verbalizer 參數：
\begin{equation}
\mathcal{L}_{CE} = - \sum_{c=1}^{C} y_c \log(\hat{y}_c)
\end{equation}
其中 $C$ 為類別總數。對於序列生成任務（如 ASR），模型則採用自回歸方式最大化目標序列的條件機率。其損失函數定義為：
\begin{equation}
\mathcal{L}_{Gen} = - \sum_{t=1}^{M} \log P(y_t | y_{<t}, X; \theta_{prompt})
\end{equation}
在此過程中，模型透過最大化似然函數學習如何根據 Prompt 上下文生成符合目標的單元序列。

Chang 等人（2023, 2024）的研究展示了 SpeechPrompt 架構的廣泛適用性。除了本計畫關注的語音分類任務外，此架構亦透過將任務轉化為序列生成問題，成功應用於自動語音辨識 (ASR)、語音翻譯 (ST) 與語音延續 (Speech Continuation) 等生成式任務，證實了 Prompting 典範作為通用語音處理框架的靈活性與潛力。

針對語音分類領域，Chang 等人（2023）在 SpeechPrompt v2 的研究中進行了廣泛評估。為了驗證此架構的潛力與適用邊界，我們引用其實驗數據（如圖三所示）進行詳細分析。

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/SpeechPrompt_result.png}
    \caption{SpeechPrompt 在各式分類任務上的表現}
\end{figure}

根據圖三的實驗數據，我們可以將 SpeechPrompt v2 在不同任務上的表現歸納為三個層次，這證明了 SpeechPrompt 是一個具備高度前景的通用架構，但也存在特定的改進空間：

\begin{enumerate}
    \item \textbf{超越 SOTA (Outperform SOTA)}：
    在某些任務中，SpeechPrompt 甚至擊敗了全參數微調的專用模型，展現了 GSLM 在語意理解上的強大優勢。
    \begin{itemize}
        \item \textbf{具體任務}：立陶宛語關鍵詞偵測 (Lithuanian SCR)、阿拉伯語關鍵詞偵測 (Arabic SCR)、以及諷刺偵測 (Sarcasm Detection)。
        \item \textbf{意涵}：這顯示 GSLM 預訓練所學到的高階語言特徵，對於語意理解與低資源語言的適應性極佳。Prompting 技術能夠有效激發這些潛在知識，在少量數據下達成超越傳統方法的表現。
    \end{itemize}

    \item \textbf{比肩 SOTA (Competitive with SOTA)}：
    在主流的標準數據集上，SpeechPrompt 僅需訓練極少量的參數（通常小於模型總參數的 0.1\%），即可達到與訓練所有參數（Fine-tuning）相當的水準。
    \begin{itemize}
        \item \textbf{具體任務}：Google Speech Commands (SCR)、語者意圖分類 (Intent Classification)、語言辨識 (LID)、性別識別 (GID) 及語音活動偵測 (VAD)。
        \item \textbf{意涵}：這證明了 SpeechPrompt 是一個真實可行且高效的解決方案。它大幅降低了儲存與計算成本，卻未顯著犧牲性能，具備極高的實用價值與參數效率（Parameter Efficiency）。
    \end{itemize}

    \item \textbf{落後 SOTA (Underperform SOTA)}：
    在涉及細微聲學特徵或非語意內容的任務上，SpeechPrompt 與專用模型仍有明顯差距。
    \begin{itemize}
        \item \textbf{具體任務}：\textbf{偽造語音偵測 (Fake Speech Detection / ASVspoof)}、情緒辨識 (Emotion Recognition)、口音分類 (Accent Classification)。
        \item \textbf{關鍵數據}：特別是在本計畫關注的 \textbf{ASVspoof 2019 LA (Logical Access)} 任務上，當時SOTA 模型的等錯誤率 (EER) 可達 \textbf{2.5\%}，而 SpeechPrompt v2 最佳僅能達到 \textbf{13.5\%}。
        \item \textbf{意涵}：這顯示出目前的 SpeechPrompt 架構在捕捉「非語意」的聲學細節（如合成語音的偽造痕跡或情緒的細微變化）時仍有局限。這可能歸因於 GSLM 前端的離散化過程（Quantization）過濾掉了部分關鍵的高頻聲學訊息，或是目前的 Prompt 機制尚未能有效引導模型關注這些特徵。
    \end{itemize}
\end{enumerate}


\subsection{模型比較與本計畫定位}

綜合上述分析，我們可以將 SpeechPrompt 與現有的 SOTA 模型進行詳細的參數與性能對比，如下表 1 所示：

\begin{table}[H]
\centering
\caption{不同架構之性能與參數比較表。SpeechPrompt 的參數計算基於 Prompt Length $L=5$。}
\label{tab:model_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{模型架構} & \textbf{範式 } & \textbf{可訓練參數量} & \textbf{ASVspoof 2019 LA (EER)} \\
\midrule
\textbf{SSL AASIST} & Pre-train, Fine-tune & 318M & 0.21\% \\
\textbf{MFA-Conformer} & Pre-train, Fine-tune & 14M & 0.72\% \\
\textbf{SpeechPrompt v2} & Prompting Paradigm & 0.13M & 13.5\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{SSL AASIST}：雖然能達到極致的性能表現，但代價是必須微調整個 wav2vec 2.0 XLS-R 巨型模型（約 318M 參數）。這使得訓練成本極高，且每個新任務都需要儲存一個完整的模型副本。
    \item \textbf{MFA-Conformer}：參考[?]的作法和實驗數據，整體參數量大幅縮減至 14M，同時保持了相當優異的偵測能力。
    \item \textbf{SpeechPrompt}：該架構在參數效率上達到了極致。根據 Prompt Tuning 的機制，我們僅需訓練 Input Prompts 與 Deep Prompts。假設 Prompt Length 為 5，GSLM (12層, 1024維) 的可訓練參數約為：
    \[ 
    (2 \times 12 \text{ (Deep layers)} + 1 \text{ (Input layer)}) \times 5 \times 1024 + \text{Verbalizer} \approx 128,000 \approx \textbf{0.13M}
    \]
    這僅是SpeechPrompt v2 總參數量的 0.8\%，SSL AASIST 可訓練參數量的 0.04\%，MFA-Conformer 的 1\%。
    \item \textbf{研究價值}：SpeechPrompt 架構在語意相關任務上展現了顯著的潛力，證明了其作為通用語音處理框架的可行性。然而，其在 \textbf{ASVspoof} 任務上的性能落差（Underperform），正是本研究欲探討的核心問題之一。我們將透過實驗探討其性能瓶頸是源於模型架構、特徵損失，還是 Prompt 的設計機制，並試圖提出可能的改進方向。
\end{itemize}
